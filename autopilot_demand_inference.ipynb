{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Water Demand Prediction with Amazon SageMaker Autopilot\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "This notebook's CI test result for ap-southeast-2 is as follows. CI test results in other regions can be found at the end of the notebook. \n",
    "\n",
    "![This us-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-2/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Using Autopilot to Predict House Prices in California**_\n",
    "\n",
    "\n",
    "Kernel `Python 3 (Data Science)` works well with this notebook. You will have the best experience running this within SageMaker Studio.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. [Introduction](#Introduction)\n",
    "1. [Setup](#Setup)\n",
    "1. [Prepare Training Data](#Data)\n",
    "1. [Train](#Settingup)\n",
    "1. [Autopilot Results](#Results)\n",
    "1. [Evaluate Using Test Data](#Evaluate)\n",
    "1. [Cleanup](#Cleanup)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Amazon SageMaker Autopilot is an automated machine learning (commonly referred to as AutoML) solution for tabular datasets. You can use SageMaker Autopilot in different ways: on autopilot (without any human input) or with human guidance, without code through SageMaker Studio or scripted using the AWS SDKs. This notebook will use the AWS SDKs to simply create and deploy a machine learning model without doing any feature engineering manually. We will also explore the auto-generated feature importance report.\n",
    "\n",
    "Demand modelling is done per location. Features for demand forecasting:\n",
    "\n",
    "* ```doy```\n",
    "* ```Aquifer```\n",
    "* ```PE```\n",
    "* ```month```\n",
    "* ```mday```\n",
    "* ```is_holiday```\n",
    "* ```wday```\n",
    "* ```API```\n",
    "* ```lagAPI```\n",
    "* ```Tmax```\n",
    "* ```Tmaxlag1```\n",
    "* ```Sun```\n",
    "* ```Sunlag1```\n",
    "* ```WVRain```\n",
    "* ```KerburnRain```\n",
    "* ```Rain_L7DAYS```\n",
    "* ```Rain_L6DAYS```\n",
    "* ```Rain_L5DAYS```\n",
    "* ```Rain_L4DAYS```\n",
    "* ```Rain_L3DAYS```\n",
    "* ```Rain_L2DAYS```\n",
    "* ```Rain_L1DAYS```\n",
    "* ```Season```\n",
    "* ```Rainlag1```\n",
    "* ```Rainlag2```\n",
    "* ```Rainlag3```\n",
    "* ```PElag1```\n",
    "* ```PElag2```\n",
    "* ```PElag3```\n",
    "* ```ANcyc```\n",
    "* ```storage```\n",
    "* ```Storagelag1```\n",
    "* ```(site name) fTemp```\n",
    "* ```(site name) fPrecp```\n",
    "* ```(site name) cm```\n",
    "* ```(site name) stat```\n",
    "* ```Restriction level```\n",
    "* ```site name```(target)\n",
    "\n",
    "What we're going to try to predict is the site water demand for a wellington region. We will let Autopilot perform feature engineering, model selection, model tuning, and give us the best candidate model ready to use for inferences.\n",
    "\n",
    "---\n",
    "## Setup\n",
    "\n",
    "_This notebook was created and tested on a ml.m5.large notebook instance._\n",
    "\n",
    "Let's start by specifying:\n",
    "\n",
    "- The S3 bucket and prefix to use for training and model data.  This should be within the same region as the Notebook Instance, training, and hosting. The following code will use SageMaker's default S3 bucket (and create one if it doesn't exist).\n",
    "- The IAM role ARN used to give training and hosting access to your data. See the documentation for how to create these. The following code will use the SageMaker execution role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## declare functions needed for job scheduling later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def convert_input_to_output_key(input_key):\n",
    "    # Split the input key by '/'\n",
    "    parts = input_key.split('/')\n",
    "    # Remove the first part 'TransformedInputs'\n",
    "    parts.pop(0)\n",
    "    # Remove the last part which is the file name\n",
    "    parts.pop()\n",
    "    # Join the remaining parts to form the output key\n",
    "    output_key = '/'.join(parts)\n",
    "    return output_key\n",
    "\n",
    "def extract_number(input_string):\n",
    "    # Use regular expression to find the number before '.csv'\n",
    "    match = re.search(r'_(\\d+)\\.csv$', input_string)\n",
    "    if match:\n",
    "        return int(match.group(1))\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def schedule_job(best_candidate_name, input_chunk_key, output_chunk_key, instance_type, bucket_name):\n",
    "    # Create a SageMaker session\n",
    "    sagemaker_session = sagemaker.Session()\n",
    "    timestamp_suffix = strftime(\"%Y%m%d-%H%M%S\", gmtime())\n",
    "    chunk_idx = extract_number(input_chunk_key)\n",
    "    transform_job_name=f'{best_candidate_name}-c{chunk_idx}-' + timestamp_suffix\n",
    "    print(f\"BatchTransformJob ({instance_type}): {transform_job_name} on {input_chunk_key}\")\n",
    "    print(f\"BatchTransformJob output: {output_chunk_key}\")\n",
    "    input_prefix = input_chunk_key\n",
    "    output_prefix = output_chunk_key\n",
    "    \n",
    "    response = sm.create_transform_job(\n",
    "        TransformJobName=transform_job_name, \n",
    "        ModelName=best_candidate_name,\n",
    "        MaxPayloadInMB=20,\n",
    "        BatchStrategy=\"MultiRecord\",\n",
    "        ModelClientConfig={\n",
    "            'InvocationsTimeoutInSeconds': 3600\n",
    "        },\n",
    "        TransformInput={\n",
    "            'DataSource': {\n",
    "                'S3DataSource': {\n",
    "                    'S3DataType': 'S3Prefix',\n",
    "                    'S3Uri': 's3://{}/{}'.format(bucket_name, input_prefix)\n",
    "                }\n",
    "            },\n",
    "            'ContentType': 'text/csv',\n",
    "            'SplitType': 'Line'\n",
    "        },\n",
    "        TransformOutput={\n",
    "            'S3OutputPath': 's3://{}/{}'.format(bucket_name, output_prefix),\n",
    "            'AssembleWith': 'Line',\n",
    "        },\n",
    "        TransformResources={\n",
    "            'InstanceType': instance_type, #'ml.c5.4xlarge', 'ml.m5.12xlarge',\n",
    "            'InstanceCount': 1\n",
    "        }\n",
    "        )\n",
    "    return transform_job_name\n",
    "\n",
    "def check_job_status(transform_job_name):\n",
    "    while True:\n",
    "        describe_response = sm.describe_transform_job(TransformJobName=transform_job_name)\n",
    "        job_run_status = describe_response[\"TransformJobStatus\"]\n",
    "        if job_run_status in (\"Failed\", \"Completed\", \"Stopped\"):\n",
    "            print(f\"{datetime.datetime.now()} {describe_response['TransformJobStatus']}\")\n",
    "            break\n",
    "        print(f\"{datetime.datetime.now()} {describe_response['TransformJobStatus']}\")\n",
    "        sleep(60)\n",
    "\n",
    "def get_job_status(job):\n",
    "    describe_response = sm.describe_transform_job(TransformJobName=job)\n",
    "    job_run_status = describe_response[\"TransformJobStatus\"]\n",
    "    return job_run_status\n",
    "\n",
    "# def schedule_batch_transform_jobs(best_candidate_name, input_chunk_key_list, output_chunk_key_list, instance_types, bucket_name):\n",
    "#     running_jobs = {instance_type: [] for instance_type in instance_types}\n",
    "#     max_parallel_jobs = 4\n",
    "#     input_index = 0\n",
    "\n",
    "#     while input_index < len(input_chunk_key_list):\n",
    "#         for instance_type in instance_types:\n",
    "#             while len(running_jobs[instance_type]) < max_parallel_jobs and input_index < len(input_chunk_key_list):\n",
    "#                 # Schedule a new job\n",
    "#                 input_chunk_key = input_chunk_key_list[input_index]\n",
    "#                 output_chunk_key = output_chunk_key_list[input_index]\n",
    "#                 try:\n",
    "#                     transform_job_name = schedule_job(best_candidate_name, input_chunk_key, output_chunk_key, instance_type, bucket_name)\n",
    "#                     running_jobs[instance_type].append(transform_job_name)\n",
    "#                     input_index += 1\n",
    "#                 except:\n",
    "#                     print(f\"schedule job exception with {instance_type}, switch to the next instance_type\")\n",
    "#                     break\n",
    "\n",
    "#         # Check the status of running jobs and remove completed jobs from the list\n",
    "#         all_running_jobs = []\n",
    "#         for instance_type in instance_types:\n",
    "#             for job in running_jobs[instance_type]:\n",
    "#                 check_job_status(job)\n",
    "#             job_status_x = [job for job in running_jobs[instance_type] if get_job_status(job) not in (\"Failed\", \"Completed\", \"Stopped\")]\n",
    "#             running_jobs[instance_type] = job_status_x\n",
    "#             all_running_jobs.extend(job_status_x)\n",
    "        \n",
    "#         if len(all_running_jobs) > 0:\n",
    "#             sleep(60)  # jobs running, Wait before checking again\n",
    "#         else:\n",
    "#             sleep(1)  # no jobs running, don't sleep\n",
    "\n",
    "#     # Wait for all remaining jobs to complete\n",
    "#     for instance_type in instance_types:\n",
    "#         while running_jobs[instance_type]:\n",
    "#             for job in running_jobs[instance_type]:\n",
    "#                 check_job_status(job)\n",
    "#             job_status_x = [job for job in running_jobs[instance_type] if get_job_status(job) not in (\"Failed\", \"Completed\", \"Stopped\")]\n",
    "#             running_jobs[instance_type] = job_status_x\n",
    "#             if len(job_status_x) > 0:\n",
    "#                 sleep(60)  # jobs running, Wait before checking again\n",
    "#             else:\n",
    "#                 sleep(1)  # no jobs running, don't sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import StringIO\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "def list_csv_files(bucket_name, key_path):\n",
    "    csv_files = []\n",
    "    paginator = s3.get_paginator('list_objects_v2')\n",
    "    page_iterator = paginator.paginate(Bucket=bucket_name, Prefix=key_path)\n",
    "    \n",
    "    for page in page_iterator:\n",
    "        if 'Contents' in page:\n",
    "            for content in page['Contents']:\n",
    "                if content['Key'].endswith('.csv'):\n",
    "                    csv_files.append(content['Key'])\n",
    "    \n",
    "    return csv_files\n",
    "\n",
    "def read_csv_files_to_dataframes(bucket_name, csv_files):\n",
    "    dataframes = []\n",
    "    for key in csv_files:\n",
    "        # Get the object from S3\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=key)\n",
    "        # Read the CSV file content\n",
    "        data = obj['Body'].read().decode('utf-8')\n",
    "        # Convert to DataFrame\n",
    "        df = pd.read_csv(StringIO(data))\n",
    "        dataframes.append(df)\n",
    "    return dataframes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# declare all auto_ml_job already completed in Canvas\n",
    "auto_ml_job_dict = {\n",
    "    'NorthWellingtonMoa': 'Canvas1734649444174',\n",
    "    'WellingtonLowLevel': 'Canvas1734648978161',\n",
    "    'Petone': 'Canvas1733434154045',\n",
    "    'WellingtonHighWestern': 'Canvas1733085655509',\n",
    "    'WellingtonHighMoa': 'Canvas1733372214860',\n",
    "    'NorthWellingtonPorirua': 'Canvas1733369877242',\n",
    "    'Porirua': 'Canvas1733437572452',\n",
    "    'Wainuiomata': 'Canvas1734649248674',\n",
    "    'UpperHutt': 'Canvas1734649294393',\n",
    "    'LowerHutt': 'Canvas1734649384856'\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launching the SageMaker Autopilot Job<a name=\"Launching\"></a>\n",
    "\n",
    "You can now launch the Autopilot job by calling the `create_auto_ml_job` API. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model on previously unseen data, we will test it against the test dataset we prepared earlier. For that, we don't necessarily need to deploy the model to an endpoint, we can simply run a batch transform job to get predictions for our unlabeled test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use models already trained Canvas for Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data prep, find simulation folders, find site folders under the respective simulation folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "from io import StringIO\n",
    "\n",
    "# Initialize the S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Define the S3 bucket and prefix\n",
    "bucket_name = 'niwa-water-demand-modelling'\n",
    "prefix = 'InferenceData/'\n",
    "\n",
    "# List objects in the specified S3 path\n",
    "response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "\n",
    "# Loop through the objects and look for CSV files containing \"Final_\"\n",
    "csv_files = []\n",
    "for obj in response.get('Contents', []):\n",
    "    key = obj['Key']\n",
    "    if key.endswith('.csv'):\n",
    "        csv_files.append(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['InferenceData/LowerHutt/Lower Hutt.csv',\n",
       " 'InferenceData/NorthWellingtonMoa/North Wellington Moa.csv',\n",
       " 'InferenceData/NorthWellingtonPorirua/North Wellington Porirua.csv',\n",
       " 'InferenceData/Petone/Petone.csv',\n",
       " 'InferenceData/Porirua/Porirua.csv',\n",
       " 'InferenceData/UpperHutt/Upper Hutt.csv',\n",
       " 'InferenceData/Wainuiomata/Wainuiomata.csv',\n",
       " 'InferenceData/WellingtonHighMoa/Wellington High Moa.csv',\n",
       " 'InferenceData/WellingtonHighWestern/Wellington High Western.csv',\n",
       " 'InferenceData/WellingtonLowLevel/Wellington Low Level.csv']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "16\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# # Example usage\n",
    "# bucket_name = 'niwa-water-demand-modelling'\n",
    "# key_path = 'TransformedOutputs/Simulation/'\n",
    "# target_files = list_csv_files(bucket_name, key_path)\n",
    "# for key in list(auto_ml_job_dict.keys()):\n",
    "#     key = f\"/{key}/\"\n",
    "#     key_inputs = [e for e in csv_files if key in e]\n",
    "#     key_files = [\"/\".join(e.split(\"/\")[1:]) for e in target_files if key in e]\n",
    "#     unfinished = [e for e in key_inputs if e not in key_files]\n",
    "#     print(f\"{key}: {len(key_files)}\")\n",
    "#     # find out which input file is not covered\n",
    "#     print(f\"{key}: {len(unfinished)} files not processed: {unfinished}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create empty target files\n",
    "target_files = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop through all sites and their Canvas model, create_model to register the model if needed, find all inference files for that site, clean up the csv file by only selecting necessary columns, remove the header. After batch transform job completes, it will save to the destination s3 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_batch_transform_jobs(best_candidate_name_list, input_chunk_key_list, output_chunk_key_list, instance_types, bucket_name):\n",
    "    running_jobs = {instance_type: [] for instance_type in instance_types}\n",
    "    max_parallel_jobs = 4\n",
    "    input_index = 0\n",
    "\n",
    "    while input_index < len(input_chunk_key_list):\n",
    "        for instance_type in instance_types:\n",
    "            while len(running_jobs[instance_type]) < max_parallel_jobs and input_index < len(input_chunk_key_list):\n",
    "                # Schedule a new job\n",
    "                input_chunk_key = input_chunk_key_list[input_index]\n",
    "                output_chunk_key = output_chunk_key_list[input_index]\n",
    "                best_candidate_name = best_candidate_name_list[input_index]\n",
    "                try:\n",
    "                    transform_job_name = schedule_job(\n",
    "                        best_candidate_name, \n",
    "                        input_chunk_key, \n",
    "                        output_chunk_key, \n",
    "                        instance_type, \n",
    "                        bucket_name\n",
    "                    )\n",
    "                    running_jobs[instance_type].append(transform_job_name)\n",
    "                    input_index += 1\n",
    "                except:\n",
    "                    print(f\"schedule job exception with {instance_type}, switch to the next instance_type\")\n",
    "                    break\n",
    "\n",
    "        # Check the status of running jobs and remove completed jobs from the list\n",
    "        all_running_jobs = []\n",
    "        for instance_type in instance_types:\n",
    "            for job in running_jobs[instance_type]:\n",
    "                check_job_status(job)\n",
    "            job_status_x = [job for job in running_jobs[instance_type] if get_job_status(job) not in (\"Failed\", \"Completed\", \"Stopped\")]\n",
    "            running_jobs[instance_type] = job_status_x\n",
    "            all_running_jobs.extend(job_status_x)\n",
    "        \n",
    "        if len(all_running_jobs) > 0:\n",
    "            sleep(60)  # jobs running, Wait before checking again\n",
    "        else:\n",
    "            sleep(1)  # no jobs running, don't sleep\n",
    "\n",
    "    # Wait for all remaining jobs to complete\n",
    "    for instance_type in instance_types:\n",
    "        while running_jobs[instance_type]:\n",
    "            for job in running_jobs[instance_type]:\n",
    "                check_job_status(job)\n",
    "            job_status_x = [job for job in running_jobs[instance_type] if get_job_status(job) not in (\"Failed\", \"Completed\", \"Stopped\")]\n",
    "            running_jobs[instance_type] = job_status_x\n",
    "            if len(job_status_x) > 0:\n",
    "                sleep(60)  # jobs running, Wait before checking again\n",
    "            else:\n",
    "                sleep(1)  # no jobs running, don't sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Canvas1734649444174-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/NorthWellingtonMoa/North Wellington Moa.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/NorthWellingtonMoa/North Wellington Moa_0.csv\n",
      "Model Canvas1734648978161-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/WellingtonLowLevel/Wellington Low Level.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/WellingtonLowLevel/Wellington Low Level_0.csv\n",
      "Model Canvas1733434154045-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/Petone/Petone.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/Petone/Petone_0.csv\n",
      "Model Canvas1733085655509-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/WellingtonHighWestern/Wellington High Western.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/WellingtonHighWestern/Wellington High Western_0.csv\n",
      "Model Canvas1733372214860-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/WellingtonHighMoa/Wellington High Moa.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/WellingtonHighMoa/Wellington High Moa_0.csv\n",
      "Model Canvas1733369877242-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/NorthWellingtonPorirua/North Wellington Porirua.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua_0.csv\n",
      "Model Canvas1733437572452-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/NorthWellingtonPorirua/North Wellington Porirua.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua_0.csv\n",
      "s3://niwa-water-demand-modelling/InferenceData/Porirua/Porirua.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/Porirua/Porirua_0.csv\n",
      "Model Canvas1734649248674-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/Wainuiomata/Wainuiomata.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/Wainuiomata/Wainuiomata_0.csv\n",
      "Model Canvas1734649294393-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/UpperHutt/Upper Hutt.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/UpperHutt/Upper Hutt_0.csv\n",
      "Model Canvas1734649384856-trial-t1-1 exists. Loading the model.\n",
      "s3://niwa-water-demand-modelling/InferenceData/LowerHutt/Lower Hutt.csv\n",
      "Uploaded chunk 0 to s3://niwa-water-demand-modelling/TransformedInputs/InferenceData/LowerHutt/Lower Hutt_0.csv\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import datetime\n",
    "from io import StringIO\n",
    "import io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# from sagemaker.model import Model\n",
    "from sagemaker import get_execution_role\n",
    "from time import gmtime, strftime, sleep\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "role = get_execution_role()\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "# Initialize the SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker')\n",
    "\n",
    "# This is the client we will use to interact with SageMaker Autopilot\n",
    "sm = boto3.Session().client(service_name=\"sagemaker\", region_name=region)\n",
    "input_file_list = []\n",
    "output_file_list = []\n",
    "best_candidate_name_list = []\n",
    "\n",
    "for site_name, auto_ml_job_name in list(auto_ml_job_dict.items()):\n",
    "    # Describe the AutoML job using the V2 API\n",
    "    # auto_ml_job_name_1 = \"Canvas1734649444174\"\n",
    "    response = sagemaker_client.describe_auto_ml_job_v2(AutoMLJobName=auto_ml_job_name)\n",
    "    \n",
    "    # Extract the best candidate details\n",
    "    best_candidate = response['BestCandidate']\n",
    "    best_candidate_name = best_candidate['CandidateName']\n",
    "    model_artifacts = best_candidate['InferenceContainers'][0]['ModelDataUrl']\n",
    "    image_uri = best_candidate['InferenceContainers'][0]['Image']\n",
    "    best_candidate_containers = best_candidate['InferenceContainers'] \n",
    "\n",
    "    # check if model exist\n",
    "    try:\n",
    "        response = sm.describe_model(ModelName=best_candidate_name)\n",
    "        print(f\"Model {best_candidate_name} exists. Loading the model.\")\n",
    "    # Load the model logic here\n",
    "    except ClientError as e:\n",
    "        if e.response['Error']['Code'] == 'ValidationException':\n",
    "            print(f\"Model {best_candidate_name} does not exist. Creating the model.\")\n",
    "            response = sm.create_model(\n",
    "                ModelName=best_candidate_name,\n",
    "                ExecutionRoleArn=role,\n",
    "                Containers=best_candidate_containers\n",
    "            )\n",
    "            print(f\"Model {best_candidate_name} created successfully.\")\n",
    "        else:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "\n",
    "    # Find all s3 paths for input files\n",
    "    input_files = [e for e in csv_files if site_name in e]\n",
    "    # Find all s3 paths for output files\n",
    "    result_files = [e for e in target_files if site_name in e]\n",
    "\n",
    "    for csv_file in input_files:\n",
    "        # check if input already has output file generate\n",
    "        result_found = [j for j in result_files if csv_file in j]\n",
    "        if len(result_found)>0:\n",
    "            print(f\"result found for {csv_file}, continue to next\")\n",
    "            continue\n",
    "                \n",
    "        print(f\"s3://{bucket_name}/{csv_file}\")\n",
    "        obj = s3.get_object(Bucket=bucket_name, Key=csv_file)\n",
    "        # Read the CSV file content\n",
    "        data = obj['Body'].read().decode('utf-8')\n",
    "        # Convert to DataFrame\n",
    "        df = pd.read_csv(StringIO(data))\n",
    "        target = csv_file.split(\"/\")[-1].split(\".csv\")[0]\n",
    "        columns = [e for e in df.columns if e not in [\"Date\", target, \"replicate\"]]\n",
    "        \n",
    "        # Upload the CSV string to S3\n",
    "        file_name = csv_file.split(\"/\")[-1]\n",
    "        \n",
    "        # Split the data into smaller chunks\n",
    "        chunk_size = 5000  # Adjust the chunk size as needed\n",
    "        chunks = [df[i:i + chunk_size] for i in range(0, df.shape[0], chunk_size)]\n",
    "\n",
    "        # Upload the chunks to S3\n",
    "        model_name_list = []\n",
    "        input_chunk_key_list = []\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            csv_buffer = StringIO()\n",
    "            chunk[columns].to_csv(csv_buffer, index=False, header=False)\n",
    "            chunk_csv_path = csv_file.replace(\".csv\", f\"_{idx}.csv\")\n",
    "            chunk_key = f\"TransformedInputs/{chunk_csv_path}\"\n",
    "            s3.put_object(Bucket=bucket_name, Key=chunk_key, Body=csv_buffer.getvalue())\n",
    "            print(f\"Uploaded chunk {idx} to s3://{bucket_name}/{chunk_key}\")\n",
    "            input_chunk_key_list.append(chunk_key)\n",
    "            model_name_list.append(best_candidate_name)\n",
    "        input_file_list.extend(input_chunk_key_list)\n",
    "        # schedule jobs for all chunked csvs\n",
    "        output_chunk_key_list = []\n",
    "        for e in input_chunk_key_list:\n",
    "            output_chunk_key_1 = convert_input_to_output_key(e)\n",
    "            output_chunk_key = f\"TransformedOutputs/{output_chunk_key_1}\"\n",
    "            output_chunk_key_list.append(output_chunk_key)\n",
    "        output_file_list.extend(output_chunk_key_list)\n",
    "        best_candidate_name_list.extend(model_name_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Canvas1734649444174-trial-t1-1',\n",
       "  'Canvas1734648978161-trial-t1-1',\n",
       "  'Canvas1733434154045-trial-t1-1',\n",
       "  'Canvas1733085655509-trial-t1-1',\n",
       "  'Canvas1733372214860-trial-t1-1',\n",
       "  'Canvas1733369877242-trial-t1-1',\n",
       "  'Canvas1733437572452-trial-t1-1',\n",
       "  'Canvas1733437572452-trial-t1-1',\n",
       "  'Canvas1734649248674-trial-t1-1',\n",
       "  'Canvas1734649294393-trial-t1-1',\n",
       "  'Canvas1734649384856-trial-t1-1'],\n",
       " ['TransformedInputs/InferenceData/NorthWellingtonMoa/North Wellington Moa_0.csv',\n",
       "  'TransformedInputs/InferenceData/WellingtonLowLevel/Wellington Low Level_0.csv',\n",
       "  'TransformedInputs/InferenceData/Petone/Petone_0.csv',\n",
       "  'TransformedInputs/InferenceData/WellingtonHighWestern/Wellington High Western_0.csv',\n",
       "  'TransformedInputs/InferenceData/WellingtonHighMoa/Wellington High Moa_0.csv',\n",
       "  'TransformedInputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua_0.csv',\n",
       "  'TransformedInputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua_0.csv',\n",
       "  'TransformedInputs/InferenceData/Porirua/Porirua_0.csv',\n",
       "  'TransformedInputs/InferenceData/Wainuiomata/Wainuiomata_0.csv',\n",
       "  'TransformedInputs/InferenceData/UpperHutt/Upper Hutt_0.csv',\n",
       "  'TransformedInputs/InferenceData/LowerHutt/Lower Hutt_0.csv'],\n",
       " ['TransformedOutputs/InferenceData/NorthWellingtonMoa',\n",
       "  'TransformedOutputs/InferenceData/WellingtonLowLevel',\n",
       "  'TransformedOutputs/InferenceData/Petone',\n",
       "  'TransformedOutputs/InferenceData/WellingtonHighWestern',\n",
       "  'TransformedOutputs/InferenceData/WellingtonHighMoa',\n",
       "  'TransformedOutputs/InferenceData/NorthWellingtonPorirua',\n",
       "  'TransformedOutputs/InferenceData/NorthWellingtonPorirua',\n",
       "  'TransformedOutputs/InferenceData/Porirua',\n",
       "  'TransformedOutputs/InferenceData/Wainuiomata',\n",
       "  'TransformedOutputs/InferenceData/UpperHutt',\n",
       "  'TransformedOutputs/InferenceData/LowerHutt'])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_candidate_name_list, input_file_list, output_file_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BatchTransformJob (ml.c5.xlarge): Canvas1734649444174-trial-t1-1-c0-20250203-005944 on TransformedInputs/InferenceData/NorthWellingtonMoa/North Wellington Moa_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/NorthWellingtonMoa\n",
      "BatchTransformJob (ml.c5.xlarge): Canvas1734648978161-trial-t1-1-c0-20250203-005945 on TransformedInputs/InferenceData/WellingtonLowLevel/Wellington Low Level_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/WellingtonLowLevel\n",
      "BatchTransformJob (ml.c5.xlarge): Canvas1733434154045-trial-t1-1-c0-20250203-005946 on TransformedInputs/InferenceData/Petone/Petone_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/Petone\n",
      "BatchTransformJob (ml.c5.xlarge): Canvas1733085655509-trial-t1-1-c0-20250203-005947 on TransformedInputs/InferenceData/WellingtonHighWestern/Wellington High Western_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/WellingtonHighWestern\n",
      "BatchTransformJob (ml.m5.2xlarge): Canvas1733372214860-trial-t1-1-c0-20250203-005949 on TransformedInputs/InferenceData/WellingtonHighMoa/Wellington High Moa_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/WellingtonHighMoa\n",
      "BatchTransformJob (ml.m5.2xlarge): Canvas1733369877242-trial-t1-1-c0-20250203-005950 on TransformedInputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/NorthWellingtonPorirua\n",
      "BatchTransformJob (ml.m5.2xlarge): Canvas1733437572452-trial-t1-1-c0-20250203-005951 on TransformedInputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/NorthWellingtonPorirua\n",
      "BatchTransformJob (ml.m5.2xlarge): Canvas1733437572452-trial-t1-1-c0-20250203-005952 on TransformedInputs/InferenceData/Porirua/Porirua_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/Porirua\n",
      "BatchTransformJob (ml.m5.xlarge): Canvas1734649248674-trial-t1-1-c0-20250203-005953 on TransformedInputs/InferenceData/Wainuiomata/Wainuiomata_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/Wainuiomata\n",
      "BatchTransformJob (ml.m5.xlarge): Canvas1734649294393-trial-t1-1-c0-20250203-005954 on TransformedInputs/InferenceData/UpperHutt/Upper Hutt_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/UpperHutt\n",
      "BatchTransformJob (ml.m5.xlarge): Canvas1734649384856-trial-t1-1-c0-20250203-005956 on TransformedInputs/InferenceData/LowerHutt/Lower Hutt_0.csv\n",
      "BatchTransformJob output: TransformedOutputs/InferenceData/LowerHutt\n",
      "2025-02-03 00:59:57.254083 InProgress\n",
      "2025-02-03 01:00:57.382374 InProgress\n",
      "2025-02-03 01:01:57.486150 InProgress\n",
      "2025-02-03 01:02:57.597695 InProgress\n",
      "2025-02-03 01:03:57.714124 Completed\n",
      "2025-02-03 01:03:57.742326 InProgress\n",
      "2025-02-03 01:04:57.863787 Completed\n",
      "2025-02-03 01:04:57.894267 Completed\n",
      "2025-02-03 01:04:57.920380 Completed\n",
      "2025-02-03 01:04:58.067527 Completed\n",
      "2025-02-03 01:04:58.094318 Completed\n",
      "2025-02-03 01:04:58.121715 Completed\n",
      "2025-02-03 01:04:58.148127 Completed\n",
      "2025-02-03 01:04:58.269796 Completed\n",
      "2025-02-03 01:04:58.295646 Completed\n",
      "2025-02-03 01:04:58.321001 Completed\n"
     ]
    }
   ],
   "source": [
    "# schedule jobs by master input&output file list\n",
    "instance_types = ['ml.c5.xlarge', 'ml.m5.2xlarge', 'ml.m5.xlarge']\n",
    "schedule_batch_transform_jobs(\n",
    "    best_candidate_name_list, \n",
    "    input_file_list, \n",
    "    output_file_list, \n",
    "    instance_types, \n",
    "    bucket_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## concatenate all chunk predictions and join back with datetime in original input file, this is done by site, when data from all sites are concatenated, they will be joined together to form a single output result by experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/NorthWellingtonMoa/North Wellington Moa_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/NorthWellingtonMoa/North Wellington Moa.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/WellingtonLowLevel/Wellington Low Level_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/WellingtonLowLevel/Wellington Low Level.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/Petone/Petone_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/Petone/Petone.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/WellingtonHighWestern/Wellington High Western_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/WellingtonHighWestern/Wellington High Western.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/WellingtonHighMoa/Wellington High Moa_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/WellingtonHighMoa/Wellington High Moa.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/NorthWellingtonPorirua/North Wellington Porirua.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/Porirua/Porirua_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/Porirua/Porirua.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/Wainuiomata/Wainuiomata_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/Wainuiomata/Wainuiomata.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/UpperHutt/Upper Hutt_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/UpperHutt/Upper Hutt.csv\n",
      "s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/LowerHutt/Lower Hutt_0.csv.out\n",
      "Uploaded merged prediction to s3://niwa-water-demand-modelling/TransformedOutputs/InferenceData/LowerHutt/Lower Hutt.csv\n"
     ]
    }
   ],
   "source": [
    "# post processing of output files\n",
    "for site_name, auto_ml_job_name in list(auto_ml_job_dict.items()):\n",
    "    # Find all s3 paths for input files\n",
    "    input_files = [e for e in csv_files if site_name in e]\n",
    "    for csv_file in input_files:\n",
    "        # List objects in the specified S3 output path\n",
    "        # output_chunk_key_list[0]\n",
    "        file_name = csv_file.split(\"/\")[-1]\n",
    "        input_key = f\"TransformedInputs/{csv_file}\"\n",
    "        output_key = convert_input_to_output_key(input_key) \n",
    "        output_prefix = f\"TransformedOutputs/{output_key}\"\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=output_prefix)\n",
    "        \n",
    "        # Loop through the objects and look for CSV files ends \"csv.out\"\n",
    "        output_files = []\n",
    "        for obj in response.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            if key.endswith('.csv.out'):\n",
    "                output_files.append(key)\n",
    "        # concatenate all files and save as csv format to s3\n",
    "        df_list = []\n",
    "        for output_file in sorted(output_files):\n",
    "            print(f\"s3://{bucket_name}/{output_file}\")\n",
    "            obj = s3.get_object(Bucket=bucket_name, Key=output_file)\n",
    "            # Read the CSV file content\n",
    "            data = obj['Body'].read().decode('utf-8')\n",
    "            # Convert to DataFrame\n",
    "            df = pd.read_csv(StringIO(data), names=[file_name.replace(\".csv\", \"\")])\n",
    "            df_list.append(df)\n",
    "        df_all = pd.concat(df_list, axis=0)\n",
    "        output_key = f\"{output_prefix}/{file_name}\"\n",
    "        csv_buffer = StringIO()\n",
    "        df_all.to_csv(csv_buffer, index=False)\n",
    "        s3.put_object(Bucket=bucket_name, Key=output_key, Body=csv_buffer.getvalue())\n",
    "        print(f\"Uploaded merged prediction to s3://{bucket_name}/{output_key}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.resource(\"s3\")\n",
    "s3_bucket = s3.Bucket(bucket)\n",
    "\n",
    "print(s3_bucket)\n",
    "job_outputs_prefix = \"{}/output/{}\".format(prefix, auto_ml_job_name)\n",
    "print(job_outputs_prefix)\n",
    "\n",
    "# Delete S3 objects\n",
    "s3_bucket.objects.filter(Prefix=job_outputs_prefix).delete()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then delete all the experiment and model resources created by the Autopilot experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_experiment_resources(experiment_name):\n",
    "    trials = sm.list_trials(ExperimentName=experiment_name)[\"TrialSummaries\"]\n",
    "    print(\"TrialNames:\")\n",
    "    for trial in trials:\n",
    "        trial_name = trial[\"TrialName\"]\n",
    "        print(f\"\\n{trial_name}\")\n",
    "\n",
    "        components_in_trial = sm.list_trial_components(TrialName=trial_name)\n",
    "        print(\"\\tTrialComponentNames:\")\n",
    "        for component in components_in_trial[\"TrialComponentSummaries\"]:\n",
    "            component_name = component[\"TrialComponentName\"]\n",
    "            print(f\"\\t{component_name}\")\n",
    "            sm.disassociate_trial_component(TrialComponentName=component_name, TrialName=trial_name)\n",
    "            try:\n",
    "                # comment out to keep trial components\n",
    "                sm.delete_trial_component(TrialComponentName=component_name)\n",
    "            except:\n",
    "                # component is associated with another trial\n",
    "                continue\n",
    "            # to prevent throttling\n",
    "            time.sleep(5)\n",
    "        sm.delete_trial(TrialName=trial_name)\n",
    "    sm.delete_experiment(ExperimentName=experiment_name)\n",
    "    print(f\"\\nExperiment {experiment_name} deleted\")\n",
    "\n",
    "\n",
    "def cleanup_autopilot_models(autopilot_job_name):\n",
    "    print(\"{0}:\\n\".format(autopilot_job_name))\n",
    "    response = sm.list_models(NameContains=autopilot_job_name)\n",
    "\n",
    "    for model in response[\"Models\"]:\n",
    "        model_name = model[\"ModelName\"]\n",
    "        print(f\"\\t{model_name}\")\n",
    "        sm.delete_model(ModelName=model_name)\n",
    "        # to prevent throttling\n",
    "        time.sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_experiment_resources(\"{0}-aws-auto-ml-job\".format(auto_ml_job_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleanup_autopilot_models(auto_ml_job_name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, the following code, when uncommented, will delete the local files used in this demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import glob\n",
    "import os\n",
    "\n",
    "\n",
    "def delete_local_files():\n",
    "    base_path = \"\"\n",
    "    dir_list = glob.iglob(os.path.join(base_path, \"{0}*\".format(auto_ml_job_name)))\n",
    "\n",
    "    for path in dir_list:\n",
    "        if os.path.isdir(path):\n",
    "            shutil.rmtree(path)\n",
    "\n",
    "    if os.path.exists(\"CaliforniaHousing\"):\n",
    "        shutil.rmtree(\"CaliforniaHousing\")\n",
    "\n",
    "    if os.path.exists(\"cal_housing.tgz\"):\n",
    "        os.remove(\"cal_housing.tgz\")\n",
    "\n",
    "    if os.path.exists(\"SageMakerAutopilotCandidateDefinitionNotebook.ipynb\"):\n",
    "        os.remove(\"SageMakerAutopilotCandidateDefinitionNotebook.ipynb\")\n",
    "\n",
    "    if os.path.exists(\"SageMakerAutopilotDataExplorationNotebook.ipynb\"):\n",
    "        os.remove(\"SageMakerAutopilotDataExplorationNotebook.ipynb\")\n",
    "\n",
    "    if os.path.exists(\"test_data_no_target.csv\"):\n",
    "        os.remove(\"test_data_no_target.csv\")\n",
    "\n",
    "    if os.path.exists(\"test_data.csv\"):\n",
    "        os.remove(\"test_data.csv\")\n",
    "\n",
    "    if os.path.exists(\"train_data.csv\"):\n",
    "        os.remove(\"train_data.csv\")\n",
    "\n",
    "\n",
    "## UNCOMMENT TO CLEAN UP LOCAL FILES\n",
    "# delete_local_files()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: If you enabled automatic endpoint creation, you will need to delete the endpoint manually.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook CI Test Results\n",
    "\n",
    "This notebook was tested in multiple regions. The test results are as follows, except for us-west-2 which is shown at the top of the notebook.\n",
    "\n",
    "![This us-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This us-east-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-east-2/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This us-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/us-west-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This ca-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ca-central-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This sa-east-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/sa-east-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This eu-west-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This eu-west-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-2/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This eu-west-3 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-west-3/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This eu-central-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-central-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This eu-north-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/eu-north-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This ap-southeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This ap-southeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-southeast-2/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This ap-northeast-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-1/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This ap-northeast-2 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-northeast-2/autopilot|autopilot_california_housing.ipynb)\n",
    "\n",
    "![This ap-south-1 badge failed to load. Check your device's internet connectivity, otherwise the service is currently unavailable](https://prod.us-west-2.tcx-beacon.docs.aws.dev/sagemaker-nb/ap-south-1/autopilot|autopilot_california_housing.ipynb)\n"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
